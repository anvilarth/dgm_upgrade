{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rUp2xWkdGFu"
   },
   "source": [
    "# Homework 1: Autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk8-eCJuaTrU"
   },
   "source": [
    "## Task 1. Theory (5pt)\n",
    "\n",
    "1. Consider the MADE model with a single hidden layer. The input object is $\\mathbf{x} \\in \\mathbb{R}^m$. We denote by $\\mathbf{W} \\in \\mathbb{R}^{h \\times m}$ the matrix of weights between the input and the hidden layer, and by $\\mathbf{V} \\in \\mathbb{R}^{m \\times h}$ the matrix of weights between the hidden and the output layer ($h$ is the number of neurons in the hidden layer). Let us generate the correct autoregressive masks $\\mathbf{M}_{\\mathbf{W}} \\in \\mathbb{R}^{h \\times m}$ and $\\mathbf{M}_{\\mathbf{V}} \\in \\mathbb{R}^{m \\times h}$ (the generation algorithm is given in Lecture 1) for the direct order of variables (The order of neurons is given by indices at the probabilities in the formula below. In this case, it is a direct order)\n",
    "$$\n",
    "    p(\\mathbf{x}) = p(x_1) \\cdot p(x_2 | x_1) \\cdot \\dots \\cdot p(x_m | x_{m-1}, \\dots, x_1).\n",
    "$$ \n",
    "Each mask is a binary matrix of 0 and 1. Let's introduce the matrix $\\mathbf{M} = \\mathbf{M}_{\\mathbf{V}} \\mathbf{M}_{\\mathbf{W}}$. Prove that:\n",
    "    * $\\mathbf{M}$ is strictly lower triangular (has zeros on the diagonal and above the diagonal);\n",
    "    * $\\mathbf{M}_{ij}$  is equal to the number of paths in the network graph between the output neuron $\\hat{x}_i$ and the input neuron $x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erVRXwzqPHwV"
   },
   "source": [
    "```\n",
    "your solution for task 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLGp4c5UPByO"
   },
   "source": [
    "\n",
    "2. Let's suppose we have 2 generative models for images of size $W \\times H \\times C$, where $W$ - image width, $H$ - image height, $C$ - number of channels. The first model $p_1(\\mathbf{x} | \\boldsymbol{\\theta})$ outputs a discrete distribution for each pixel  $\\text{Categorical}(\\boldsymbol{\\pi})$, где $\\boldsymbol{\\pi} = (\\pi_1, \\dots,  \\pi_{256})$. The second model $p_2(\\mathbf{x} | \\boldsymbol{\\theta})$ models a discrete distribution by a continuous mixture of logistic functions ($\\boldsymbol{\\tilde{\\pi}}$ - mixing distribution):\n",
    "\n",
    "$$\n",
    "    P(x | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}) = P(x + 0.5 | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}) - P(x - 0.5 | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p(\\nu | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\tilde{\\pi}}) = \\sum_{k=1}^K \\tilde{\\pi}_k p(\\nu | \\mu_k, s_k).\n",
    "$$\n",
    "\n",
    "Each of the models outputs parameters of pixel distributions.\n",
    "\n",
    "* Calculate the dimensions of the output tensor for the model $p_1(\\mathbf{x} | \\boldsymbol{\\theta})$ and for the model $p_2(\\mathbf{x} | \\boldsymbol{\\theta})$. \n",
    "* At what number of mixture components $K$ is the number of elements of the output tensor for $p_2(\\mathbf{x} | \\boldsymbol{\\theta})$ becomes greater than $p_1(\\mathbf{x} | \\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0HFuytJPQ7o"
   },
   "source": [
    "```\n",
    "your solution for task 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivV4DY1SPEIL"
   },
   "source": [
    "3. In the course, we will meet different divergences (not only $KL$). So let's get acquainted with the class of $\\alpha$ - divergences:\n",
    "$$\n",
    "    D_{\\alpha}(p || q) = \\frac{4}{1 - \\alpha^2} \\left( 1 - \\int p(x)^{\\frac{1 + \\alpha}{2}}q(x)^{\\frac{1 - \\alpha}{2}}dx\\right).\n",
    "$$\n",
    "For each $\\alpha \\in [-\\infty; +\\infty]$ the function $D_{\\alpha} (p || q)$ is a measure of the similarity of the two distributions, which could have different properties.\n",
    "\t  \n",
    "      Prove that for $\\alpha \\rightarrow 1$ the divergence $D_{\\alpha}(p || q) \\rightarrow KL(p || q)$, and for $\\alpha \\rightarrow -1$ the divergence $D_{\\alpha}(p || q) \\rightarrow KL(q || p)$. \n",
    "\n",
    "    **Hint:** use the fact that $t^\\varepsilon = \\exp(\\varepsilon \\cdot \\ln t) = 1 + \\varepsilon \\cdot \\ln t + O(\\varepsilon^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UP5Y6Z4PSP3"
   },
   "source": [
    "```\n",
    "your solution for task 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2LH7vKPPVUt"
   },
   "source": [
    "Now it time to move on to practical part of homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package to download from google drive\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zk6rWePvdGFv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m5NVPFaJGHO"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-wuVhhNdGFz"
   },
   "source": [
    "Use the following functions to train your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3MMoLe6dGFz"
   },
   "outputs": [],
   "source": [
    "# if you have dimension issues while training the model,\n",
    "# it is helpful to look at the implementation in the library:\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss\n",
    "def get_cross_entropy_loss(scores, labels):\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_computation_get_cross_entropy_loss():\n",
    "    input = torch.tensor([[1, 2, 3, 4],[5, 6, 7, 8]], dtype=torch.float32)\n",
    "    target = torch.tensor([3, 1], dtype=torch.long)\n",
    "\n",
    "    assert np.allclose(get_cross_entropy_loss(input, target).numpy(), 1.4402)\n",
    "\n",
    "def test_work_with_dimensions_get_cross_entropy_loss():\n",
    "    input = torch.randn(4, 3, 2, requires_grad=True)\n",
    "    target = torch.empty(4, 2, dtype=torch.long).random_(3)\n",
    "    get_cross_entropy_loss(input, target)\n",
    "\n",
    "test_computation_get_cross_entropy_loss()\n",
    "test_work_with_dimensions_get_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYg9WpssK66A"
   },
   "source": [
    "Do not change these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XzPpaW7JbLn"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, use_cuda, loss_key='total'):\n",
    "    model.train()\n",
    "  \n",
    "    stats = defaultdict(list)\n",
    "    for x in train_loader:\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        losses = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        losses[loss_key].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            stats[k].append(v.item())\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, use_cuda):\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            losses = model.loss(x)\n",
    "            for k, v in losses.items():\n",
    "                stats[k] += v.item() * x.shape[0]\n",
    "\n",
    "        for k in stats.keys():\n",
    "            stats[k] /= len(data_loader.dataset)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, use_tqdm=False, use_cuda=False, loss_key='total_loss'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    for epoch in forrange:\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n",
    "        test_loss = eval_model(model, test_loader, use_cuda)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return dict(train_losses), dict(test_losses)\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    n_train = len(train_losses[list(train_losses.keys())[0]])\n",
    "    n_test = len(test_losses[list(train_losses.keys())[0]])\n",
    "    x_train = np.linspace(0, n_test - 1, n_train)\n",
    "    x_test = np.arange(n_test)\n",
    "\n",
    "    plt.figure()\n",
    "    for key, value in train_losses.items():\n",
    "        plt.plot(x_train, value, label=key + '_train')\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        plt.plot(x_test, value, label=key + '_test')\n",
    "\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_pickle(path, flatten=True):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data = data['train'].astype('float32')[:, :, :, [0]] > 128\n",
    "    test_data = data['test'].astype('float32')[:, :, :, [0]] > 128\n",
    "    train_data = np.transpose(train_data.astype('uint8'), (0, 3, 1, 2))\n",
    "    test_data = np.transpose(test_data.astype('uint8'), (0, 3, 1, 2))\n",
    "    if flatten:\n",
    "        train_data = train_data.reshape(-1, 28 * 28)\n",
    "        test_data = test_data.reshape(-1, 28 * 28)\n",
    "    return train_data, test_data\n",
    "\n",
    "def show_samples(samples, title, nrow=10):\n",
    "    samples = torch.FloatTensor(samples).reshape(-1, 28, 28)\n",
    "    samples = torch.unsqueeze(samples, axis=1)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_mnist_images(data, title):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = train_data[idxs]\n",
    "    show_samples(images, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GLIbwqkdGGD"
   },
   "source": [
    "## Task 2: MADE on 2D data (5pt)\n",
    "\n",
    "Train MADE model on single image (see paper for details: https://arxiv.org/abs/1502.03509).\n",
    "\n",
    "You will work with bivariate data of the form $x = (x_0,x_1)$, where $x_0, x_1 \\in \\{0, \\ldots, \\text{n_bins}\\}$ (e.g. Categorial random variables). \n",
    "\n",
    "Implement and train a MADE model through MLE to represent $p(x_0, x_1)$ on the given image, with any autoregressive ordering of your choosing ($p(x_0, x_1) = p(x_0)p(x_1 | x_0)$ or $p(x_0, x_1) = p(x_1)p(x_0 | x_1)$). \n",
    "\n",
    "We advice you to think about what conditional distribution that you want to fit and how MADE's masks should look like. It may be useful to one-hot encode your inputs.\n",
    "\n",
    "You do not have to change these functions (except the path to the data file. Download the file from here: https://drive.google.com/file/d/1GUthJrA5fBpvi593Swo36t8zaFw9Dyak/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the file one can run this cell\n",
    "!gdown --id 1GUthJrA5fBpvi593Swo36t8zaFw9Dyak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngmqoNA2dGGD"
   },
   "outputs": [],
   "source": [
    "def generate_2d_data(count, bins):\n",
    "    # change the path to the image\n",
    "    im = Image.open(os.path.join('drive', 'My Drive', 'DGM', 'homework_supplementary', 'dgm.png')).resize((bins, bins)).convert('L')\n",
    "    im = np.array(im).astype('float32')\n",
    "    dist = im / im.sum()\n",
    "\n",
    "    pairs = list(itertools.product(range(bins), range(bins)))\n",
    "    idxs = np.random.choice(len(pairs), size=count, replace=True, p=dist.reshape(-1))\n",
    "    samples = np.array([pairs[i] for i in idxs])\n",
    "\n",
    "    split = int(0.8 * len(samples))\n",
    "    return dist, samples[:split], samples[split:]\n",
    "\n",
    "\n",
    "def plot_2d_data(train_data, test_data):\n",
    "    bins = int(max(test_data.max(), train_data.max()) - min(test_data.min(), train_data.min())) + 1\n",
    "    train_dist, test_dist = np.zeros((bins, bins)), np.zeros((bins, bins))\n",
    "    for i in range(len(train_data)):\n",
    "        train_dist[train_data[i][0], train_data[i][1]] += 1\n",
    "    train_dist /= train_dist.sum()\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        test_dist[test_data[i][0], test_data[i][1]] += 1\n",
    "    test_dist /= test_dist.sum()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    ax1.set_title('Train Data')\n",
    "    ax1.imshow(train_dist, cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x0')\n",
    "\n",
    "    ax2.set_title('Test Data')\n",
    "    ax2.imshow(test_dist, cmap='gray')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x0')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_2d_distribution(true_dist, learned_dist):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    ax1.imshow(true_dist, cmap='gray')\n",
    "    ax1.set_title('True Distribution')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(learned_dist, cmap='gray')\n",
    "    ax2.set_title('Learned Distribution')\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eX5p_02jdGGG"
   },
   "outputs": [],
   "source": [
    "COUNT = 20000\n",
    "BINS = 60\n",
    "\n",
    "image, train_data, test_data = generate_2d_data(COUNT, BINS)\n",
    "plot_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpcKxvPUMq27"
   },
   "source": [
    "Now we have to implement masked dense layer. It is a core component of MADE model. It acts like a usual dense layer, but firstly multiplies the weights by the predefined binary mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLs_GDnMMoZg"
   },
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(in_features, out_features)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # NOTE: only the weights are multiplied by the mask, the bias remains unchanged.\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "\n",
    "\n",
    "layer = MaskedLinear(2, 2)\n",
    "\n",
    "x = torch.tensor([1, 2], dtype=torch.float32)\n",
    "output = layer(x).detach().numpy()\n",
    "\n",
    "layer.set_mask(np.array([[0, 0], [0, 0]]))\n",
    "assert np.allclose(layer(x).detach().numpy(), layer.bias.detach().numpy())\n",
    "\n",
    "layer.set_mask(np.array([[1, 1], [1, 1]]))\n",
    "assert np.allclose(layer(x).detach().numpy(), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R--RMmRgdGGI"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, d):\n",
    "    \"\"\"\n",
    "        The function takes categorical labels of size: batch_size x n_dims.\n",
    "        One-hot encodes them to d bins and then reshapes the result to batch_size x (n_dims * d)\n",
    "\n",
    "        When OHE occurs, the neuron number is matched to the entire vector describing that neuron.\n",
    "    \"\"\"\n",
    "    assert len(labels.shape) == 2\n",
    "    one_hot = F.one_hot(labels.to(torch.int64), d)\n",
    "    return one_hot.view((labels.shape[0], -1)).float()\n",
    "\n",
    "    \n",
    "class MADE(nn.Module):\n",
    "    def __init__(self, nin, bins, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.nin = nin\n",
    "        self.nout = nin * bins\n",
    "        self.bins = bins\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        # we will use the trivial ordering of input units\n",
    "        self.ordering = np.arange(self.nin)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define a simple MLP (sequence of MaskedLinear and ReLU) neural net \n",
    "        # self.net = nn.Sequential(list of layers)\n",
    "        # do not place ReLU at the end of the network!\n",
    "        # note: the first layer of model should have nin * bins input units\n",
    "        \n",
    "        # ====\n",
    "\n",
    "        self.create_mask()  # builds the initial self.m connectivity\n",
    "\n",
    "    def get_mask(self, in_order, out_features, init_order, mask_type=None):\n",
    "\n",
    "        # point: create in_degrees, out_degrees, mask in each blocks below\n",
    "        if mask_type == 'input':\n",
    "            ### your code here\n",
    "        elif mask_type == 'output':\n",
    "            ### your code here\n",
    "        else:\n",
    "            ### your code here\n",
    "        return mask, out_degrees\n",
    "\n",
    "    def create_mask(self):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) The ordering of input units from 1 to m (self.ordering).\n",
    "        # 2) Assign the random number k from 1 to m − 1 to each hidden unit. \n",
    "        #    This number gives the maximum number of input units to which the unit can be connected.\n",
    "        # 3) Each hidden unit with number k is connected with the previous layer units \n",
    "        #   which has the number is less or equal than k.\n",
    "        # 4) Each output unit with number k is connected with the previous layer units \n",
    "        #    which has the number is less than k.\n",
    "        \n",
    "        # ====\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "\n",
    "        ## here create mask for each MaskedLinear layer\n",
    "\n",
    "        ###\n",
    "\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def visualize_masks(self):\n",
    "        prod = self.masks[0]\n",
    "        for idx, m in enumerate(self.masks):\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.title(f'layer: {idx}')\n",
    "            plt.imshow(m, vmin=0, vmax=1, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "            if idx > 0:\n",
    "                prod=prod.dot(m)\n",
    "\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.title('prod')\n",
    "        plt.imshow(prod, vmin=0, vmax=1, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.size()) == 2\n",
    "        assert x.shape[1] == self.nin\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply one hot encoding to x\n",
    "        # 2) apply the model\n",
    "        # 3) reshape (not view!) and transpose the output to (batch_size, self.bins, self.nin)\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def loss(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "        total_loss = ...\n",
    "        return {'total_loss':  total_loss}\n",
    "\n",
    "    def sample(self, n, use_cuda=True):\n",
    "        # read carefully and understand the sampling process\n",
    "        xs = []\n",
    "        for _ in range(n):\n",
    "            x = torch.randint(0, self.bins, (1, self.nin))\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            for it in range(self.nin):\n",
    "                probs = F.softmax(model(x)[0], dim=0).T\n",
    "                distr = torch.distributions.categorical.Categorical(probs)\n",
    "                x[0, it] = distr.sample()[it]\n",
    "            xs.append(x)\n",
    "        xs = torch.cat(xs)\n",
    "        return xs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WyKyynRGykw"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "HIDDEN_SIZES = # a pair of layers with a small number of neurons (4 < _ < 16) is recommended for visibility\n",
    "DIM_X = 2\n",
    "# ====\n",
    "\n",
    "model = MADE(DIM_X, BINS, HIDDEN_SIZES)\n",
    "\n",
    "\n",
    "def test_model_output(model):\n",
    "    assert [10, BINS, DIM_X] == list(model(torch.randint(0, BINS, (10, DIM_X))).size())\n",
    "\n",
    "\n",
    "def test_create_mask(model):\n",
    "    prod = np.ones((1, BINS * DIM_X))\n",
    "    for m in model.masks:\n",
    "        assert set(np.unique(m)).issubset((True, False))\n",
    "        prod = prod.dot(m)\n",
    "    assert np.allclose(prod, np.repeat(np.array([[0, BINS * np.prod(HIDDEN_SIZES)]]), BINS))\n",
    "\n",
    "# Think about why such checks are correct? What values can we have as the numbers of neurons?\n",
    "test_create_mask(model)\n",
    "test_model_output(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKam2n--SOPs"
   },
   "source": [
    "Now we will visualize the model masks. It should helps you to understand whether the model is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svPeNKPbJsTP"
   },
   "outputs": [],
   "source": [
    "model.visualize_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NWRhm_zdGGK"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# you have to choose these parameters by yourself\n",
    "HIDDEN_SIZES =  # several layers with the number of neurons from 128 to 512\n",
    "BINS = 60\n",
    "X_DIM = 2\n",
    "model = MADE(X_DIM, BINS, HIDDEN_SIZES)\n",
    "\n",
    "BATCH_SIZE =   # any adequate value\n",
    "EPOCHS =       # < 20\n",
    "LR =           # any adequate value\n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=USE_CUDA)\n",
    "\n",
    "assert test_losses[-1] < 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZRJq622LK4q"
   },
   "outputs": [],
   "source": [
    "def get_distribution(model, use_cuda=True):\n",
    "    x = np.mgrid[0:model.bins, 0:model.bins].reshape(2, model.bins ** 2).T\n",
    "    # ====\n",
    "    # your code\n",
    "    # 1) take the model output for the grid x (shape: bins ** 2, bins, 2)\n",
    "    # 3) apply log_softmax to get log probs (shape: bins ** 2, bins, 2)\n",
    "    # 4) apply torch.gather to gather vaalues indexed by grid x (shape: bins ** 2, 2)\n",
    "    # 5) sum the log probs over dim=1 (shape: bins ** 2)\n",
    "    # 6) exponentiate it (shape: bins ** 2)\n",
    "    # 7) return an array BINS x BINS with probabilities of each pixel\n",
    "\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "distribution = get_distribution(model, USE_CUDA)\n",
    "assert distribution.shape == (BINS, BINS)\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "plot_2d_distribution(image, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCRYHJALfMMk"
   },
   "outputs": [],
   "source": [
    "# draw samples from model \n",
    "\n",
    "samples = model.sample(5000)\n",
    "plot_2d_data(train_data, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdgaPdSxQqXp"
   },
   "source": [
    "## Task 3: MADE on MNIST (3pt)\n",
    "\n",
    "\n",
    "You do not have to change this functions (except the path to the data file, download it from here: https://drive.google.com/file/d/1Ms-RBybrueI3_w2CRj7lM9mYjfvFRL6w/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1Ms-RBybrueI3_w2CRj7lM9mYjfvFRL6w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHk4StQKLQ9n"
   },
   "outputs": [],
   "source": [
    "# change the path to the file\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'My Drive', 'DGM', 'homework_supplementary', 'mnist.pkl'))\n",
    "visualize_mnist_images(train_data, 'MNIST samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFyUdMB5RQJm"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "HIDDEN_SIZES = # several layers with the number of neurons from 128 to 512\n",
    "# ====\n",
    "\n",
    "BINS = 2\n",
    "\n",
    "model = MADE(28 * 28, BINS, HIDDEN_SIZES)\n",
    "\n",
    "def test_model_output(model):\n",
    "    assert [10, BINS, 28 * 28] == list(model(torch.randint(0, 2, (10, 28 * 28))).size())\n",
    "\n",
    "\n",
    "test_model_output(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5oSStf8SSf8"
   },
   "outputs": [],
   "source": [
    "# show on your masks and assure that they are correct\n",
    "model.visualize_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueL7uJPOSveZ"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# you have to choose these parameters by yourself\n",
    "BATCH_SIZE = # any adequate value\n",
    "EPOCHS =     # > 20\n",
    "LR =         # <1e-2\n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_tqdm=True, use_cuda=USE_CUDA)\n",
    "\n",
    "assert test_losses[-1] < 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvt5VHqRTI84"
   },
   "outputs": [],
   "source": [
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJDGq9OAWphh"
   },
   "outputs": [],
   "source": [
    "samples = model.sample(25)\n",
    "show_samples(samples, title='MNIST samples', nrow=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
